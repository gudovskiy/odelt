## One-Step Diffusion via Shortcut Models 

Kevin Frans, Danijar Hafner, Sergey Levine, Pieter Abbeel

[Paper Link](https://arxiv.org/abs/2410.12557)
[Website Link](https://kvfrans.com/shortcut-models/)

### Abstract
Diffusion models and flow-matching models have enabled generating diverse and realistic images by learning to transfer noise to data.
However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive.
Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling.
We introduce shortcut models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps.
Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process.
Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow.
Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time.

![Showcase Figire](data/fig-showcase4.png)

### Overview

Shortcut models can utilize standard diffusion architectures (e.g. DiT), and condition on both `t` and `d`. At `d â‰ˆ 0`, the shortcut objective is equivalent to the flow-matching objective, and can be trained by regressing onto empirical `E[vt|xt]` samples. Targets for larger `d` shortcuts are constructed by concatenating a sequence of two `d/2` shortcuts. Both objectives can be trained jointly; shortcut models do not require a two-stage procedure or discretization schedule.

![Showcase Figire](data/fig-method5.png)

### Using the code

```
conda env create -f environment.yml
pip install -U "jax[cuda12]"
pip install -r requirements.txt
```

This codebase is written in JAX, and was developed on TPU-v3 machines. You should start by installing the conda dependencies from `environment.yml` and `requirements.txt`. To load datasets, we use TFDS, and you can see our specific dataloaders at [https://github.com/kvfrans/tfds_builders](https://github.com/kvfrans/tfds_builders), of course you are free to use your own dataloader as well. 

To train a DiT-B scale model on CelebA:
```
python train.py --model.hidden_size 768 --model.patch_size 2 --model.depth 12 --model.num_heads 12 --model.mlp_ratio 4 --dataset_name celebahq256 --fid_stats data/celeba256_fidstats_ours.npz --model.cfg_scale 0 --model.class_dropout_prob 1 --model.num_classes 1 --batch_size 64 --max_steps 410_000 --model.train_type shortcut
```
or on Imagenet-256:
``` 
python train.py --model.hidden_size 768 --model.patch_size 2 --model.depth 12 --model.num_heads 12 --model.mlp_ratio 4 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --model.cfg_scale 1.5 --model.class_dropout_prob 0.1 --model.bootstrap_cfg 1 --batch_size 256 --max_steps 810_000 --model.train_type shortcut
```

A larger DiT-XL scale model can be trained via:
``` 
python train.py --model.hidden_size 1152 --model.patch_size 2 --model.depth 28 --model.num_heads 16 --model.mlp_ratio 4 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --model.cfg_scale 1.5 --model.class_dropout_prob 0.1 --model.bootstrap_cfg 1 --batch_size 256 --max_steps 810_000 --model.train_type shortcut
```

To train a regular flow model instead, use `--model.train_type naive`. This code also supports `--model.sharding fsdp` for fully-sharded data parallelism, which is recommended if you are training on a multi-GPU or TPU machine.

### Sanity Checking

Shorcut models trained with the provided functions should achieve the following FID-50k performance.

|                           | 128-Step| 4-Step  | 1-Step  |
| --------                  | ------- | ------- | ------- |
| CelebA (DiT-B)            | 6.9     | 13.8    | 20.5    |
| Imagenet-256 (DiT-B)      | 15.5    | 28.3    | 40.3    |
| Imagenet-256 (DiT-XL)     | 3.8     | 7.8     | 10.6    |

### Checkpoints and FID Stats

Pretrained model checkpoints, and pre-computed reference FID stats for CelebA and Imagenet can be downloaded from [this drive](https://drive.google.com/drive/folders/1g665i0vMxm8qqqcp5mAiexnL919-gMwW?usp=sharing). To load a checkpoint, use the `--load_dir` flag. 

### Train
```
--fid_stats data/celeba256_fidstats_ours.npz
--model.cfg_scale 0
--model.num_classes 1
--model.class_dropout_prob 1.0

python train.py --dataset_name cifar10 --batch_size 256 --max_steps 100_000 --model.train_type naive --model.use_stable_vae 0 --log_interval 1000 --eval_interval 2000
# rcfm is equivalent to "naive"
python train.py --dataset_name cifar10 --batch_size 256 --max_steps 100_000 --model.train_type rcfm  --model.use_stable_vae 0 --log_interval 1000 --eval_interval 2000 eval_interval 100_001
python train.py --dataset_name cifar10 --batch_size 256 --max_steps 100_000 --model.train_type icfm  --model.use_stable_vae 0 --log_interval 1000 --eval_interval 2000 eval_interval 100_001
python train.py --dataset_name cifar10 --batch_size 256 --max_steps 100_000 --model.train_type   fm  --model.use_stable_vae 0 --log_interval 1000 --eval_interval 2000 eval_interval 100_001
```

## Eval
```
ema=0, S=  1, 3.5it/s, FID=24.87
ema=0, S=  4, 2.9it/s, FID=17.16
ema=0, S=128, 2.6it/s, FID=15.24

ema=1, S=  1, 3.5it/s, FID=23.54
ema=1, S=  4, 2.9it/s, FID=16.44
ema=1, S=128, 2.6it/s, FID=11.22

python train.py --dataset_name celebahq256 --fid_stats data/celeba256_fidstats_ours.npz --batch_size 16 --model.train_type shortcut --load_dir ckpts/celeba-shortcut2-every4400001 --mode eval --inference_timesteps 1 --model.use_ema 1







#python train.py --batch_size 64              --model.train_type shortcut --save_dir ckpts/our-celeba-shortcut
python train.py --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts/our-celeba-shortcut --load_dir ckpts/celeba-shortcut2-every4400001 --max_steps 100000 --eval_interval 10000 --save_interval 10000

python train.py --batch_size 16 --mode eval --model.use_ema 1 --inference_solver euler  --inference_stepsize const --inference_timesteps 1
python train.py --batch_size 16 --mode eval --model.use_ema 1 --inference_solver euler  --inference_stepsize const --inference_timesteps 4
python train.py --batch_size 16 --mode eval --model.use_ema 1 --inference_solver euler  --inference_stepsize const --inference_timesteps 128

python train.py --batch_size 16 --mode eval --model.use_ema 1 --inference_solver dopri5 --inference_stepsize pid --inference_tol 1e-2
python train.py --batch_size 16 --mode eval --model.use_ema 1 --inference_solver dopri5 --inference_stepsize pid --inference_tol 1e-3
python train.py --batch_size 16 --mode eval --model.use_ema 1 --inference_solver dopri5 --inference_stepsize pid --inference_tol 1e-4

python train.py --batch_size 16 --mode eval --model.use_ema 1 --inference_solver tsit5 --inference_stepsize pid --inference_tol 1e-2
python train.py --batch_size 16 --mode eval --model.use_ema 1 --inference_solver tsit5 --inference_stepsize pid --inference_tol 1e-3
python train.py --batch_size 16 --mode eval --model.use_ema 1 --inference_solver tsit5 --inference_stepsize pid --inference_tol 1e-4

manual Euler:
[01:44<00:00,  2.44it/s] FID is 23.54/23.13
[01:59<00:00,  2.14it/s] FID is 16.44/16.48
[11:24<00:00,  0.37s/it] FID is 11.22/10.03

diffrax Euler:
[02:47<00:00,  1.53it/s] FID is 23.14
[02:57<00:00,  1.44it/s] FID is 16.50
[12:09<00:00,  0.35it/s] FID is 10.04

diffrax dopri5:
[05:03<00:00,  0.85s/it] FID is 9.67
[05:41<00:00,  0.75it/s] FID is 9.61
[07:54<00:00,  0.54it/s] FID is 9.82
[15:25<00:00,  0.28it/s] FID is 9.83

diffrax tsit5:
[05:32<00:00,  0.77s/it] FID is 9.77
[07:50<00:00,  0.55it/s] FID is 9.75
[17:50<00:00,  0.24s/it] FID is 9.81
canceled because it was slow

# CELEBA
DONE 400K:
CUDA_VISIBLE_DEVICES=2,3,     python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts/celeba-shortcut-scratch   --max_steps 1000000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000
DONE with new code/schedule:
CUDA_VISIBLE_DEVICES=2 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type   rfm --save_dir ckpts7T/celeba-rfm   --max_steps 200_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000
CUDA_VISIBLE_DEVICES=3 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type    fm --save_dir ckpts7T/celeba-fm    --max_steps 200_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000
CUDA_VISIBLE_DEVICES=6 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type  icfm --save_dir ckpts7T/celeba-icfm  --max_steps 200_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000
CUDA_VISIBLE_DEVICES=7 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type naive --save_dir ckpts7T/celeba-naive --max_steps 200_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000
NOW depthwise:
CUDA_VISIBLE_DEVICES=2 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type   rfm --save_dir ckpts7T/celeba-rfm-1ldepth   --model.depthwise 1 --max_steps 100_000 --eval_interval 20000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --load_dir ckpts7T/celeba-rfm/150000.pkl
CUDA_VISIBLE_DEVICES=3 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type    fm --save_dir ckpts7T/celeba-fm-1ldepth    --model.depthwise 1 --max_steps 100_000 --eval_interval 20000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --load_dir ckpts7T/celeba-fm/150000.pkl
CUDA_VISIBLE_DEVICES=6 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type  icfm --save_dir ckpts7T/celeba-icfm-1ldepth  --model.depthwise 1 --max_steps 100_000 --eval_interval 20000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --load_dir ckpts7T/celeba-icfm/150000.pkl
CUDA_VISIBLE_DEVICES=7 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type naive --save_dir ckpts7T/celeba-naive-1ldepth --model.depthwise 1 --max_steps 100_000 --eval_interval 20000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --load_dir ckpts7T/celeba-naive/150000.pkl
NOW baselines:
CUDA_VISIBLE_DEVICES=1 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type   rfm --save_dir ckpts7T/celeba-rfm-ft   --max_steps 100_000 --eval_interval 20000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --load_dir ckpts7T/celeba-rfm/150000.pkl
TODO baselines:
CUDA_VISIBLE_DEVICES=3 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type    fm --save_dir ckpts7T/celeba-fm-ft    --max_steps 100_000 --eval_interval 20000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --load_dir ckpts7T/celeba-fm/150000.pkl
CUDA_VISIBLE_DEVICES=6 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type  icfm --save_dir ckpts7T/celeba-icfm-ft  --max_steps 100_000 --eval_interval 20000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --load_dir ckpts7T/celeba-icfm/150000.pkl
CUDA_VISIBLE_DEVICES=7 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 64 --mode train --model.train_type naive --save_dir ckpts7T/celeba-naive-ft --max_steps 100_000 --eval_interval 20000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --load_dir ckpts7T/celeba-naive/150000.pkl




# UNCONDITIONAL CIFAR-10
python train.py --dataset_name cifar10 --batch_size 128 --max_steps 100000 --model.train_type rfm --model.use_stable_vae 0 --model.blockwise 1 --log_interval 1000 --save_interval 50000 --eval_interval 100002 --model.warmup 5000 --save_dir ckpts/cifar10-rfm-block
python train.py --dataset_name cifar10 --batch_size 128 --max_steps 20_000 --model.train_type rfm --model.use_stable_vae 0 --model.blockwise 1 --log_interval 1000 --save_interval 10000 --eval_interval 100002 --model.warmup 1000 --save_dir ckpts/cifar10-rfm-block-ft --load_dir ckpts/cifar10-rfm-block/100000.pkl

CUDA_VISIBLE_DEVICES=1,5,6,7, python train.py --dataset_name cifar10 --batch_size 512 --max_steps 100000 --model.train_type rfm --model.use_stable_vae 0 --model.blockwise 1 --log_interval 1000 --save_interval 50000 --eval_interval 100002 --model.warmup 5000 --save_dir ckpts/cifar10-rfm-block
CUDA_VISIBLE_DEVICES=1,5,6,7, python train.py --dataset_name cifar10 --batch_size 512 --max_steps 20_000 --model.train_type rfm --model.use_stable_vae 0 --model.blockwise 1 --log_interval 1000 --save_interval 10000 --eval_interval 100002 --model.warmup 1000 --model.use_cosine 1 --save_dir ckpts/cifar10-rfm-block-ft --load_dir ckpts/cifar10-rfm-block/100000.pkl


BEFORE cosine finetuning:
B4  = FID: 36.80 with 2.84 per-batch latency, sec
B8  = FID: 14.04 with 5.24 per-batch latency, sec
B12 = FID: 13.93 with 7.74 per-batch latency, sec
AFTER cosine finetuning:
B4  = FID: 33.47 with 2.84 per-batch latency, sec
B8  = FID: 12.77 with 5.27 per-batch latency, sec
B12 = FID: 12.82 with 7.76 per-batch latency, sec

python train.py --dataset_name cifar10 --batch_size 128 --mode eval --model.train_type rfm --model.use_stable_vae 0 --model.use_ema 1 --model.blockwise 1 --inference_solver dopri5 --inference_stepsize pid --inference_plot 1 --inference_tol 1e-3 --inference_generations 4096 --load_dir ckpts/cifar10-rfm-block/100000.pkl --fid_stats data/cifar10_fidstats_jax.npz 



CUDA_VISIBLE_DEVICES=0,1, python train.py --dataset_name cifar10 --batch_size 256 --max_steps 100_000 --model.train_type  rfm  --model.use_stable_vae 0 --log_interval 1000 --save_interval 20000 --eval_interval 100002 --model.warmup 5000 --save_dir ckpts7T/cifar10-rfm-test
CUDA_VISIBLE_DEVICES=2,3, python train.py --dataset_name cifar10 --batch_size 256 --max_steps 100_000 --model.train_type icfm  --model.use_stable_vae 0 --log_interval 1000 --save_interval 20000 --eval_interval 100002 --model.warmup 5000 --save_dir ckpts7T/cifar10-icfm-scratch
CUDA_VISIBLE_DEVICES=4,5, python train.py --dataset_name cifar10 --batch_size 256 --max_steps 100_000 --model.train_type   fm  --model.use_stable_vae 0 --log_interval 1000 --save_interval 20000 --eval_interval 100002 --model.warmup 5000 --save_dir ckpts7T/cifar10-fm-scratch
CUDA_VISIBLE_DEVICES=6,7, python train.py --dataset_name cifar10 --batch_size 256 --max_steps 100_000 --model.train_type naive --model.use_stable_vae 0 --log_interval 1000 --save_interval 20000 --eval_interval 100002 --model.warmup 5000 --save_dir ckpts7T/cifar10-naive128-scratch --model.denoise_timesteps 128

CUDA_VISIBLE_DEVICES=2,3, python train.py --dataset_name cifar10 --batch_size 256 --max_steps 20_000 --model.train_type  rfm  --model.use_stable_vae 0 --log_interval 1000 --save_interval 10000 --eval_interval 100002 --model.warmup 1000 --model.use_cosine 1 --save_dir ckpts7T/cifar10-rfm-ft --load_dir ckpts7T/cifar10-rfm-scratch/80000.pkl
CUDA_VISIBLE_DEVICES=6,7, python train.py --dataset_name cifar10 --batch_size 256 --max_steps 20_000 --model.train_type icfm  --model.use_stable_vae 0 --log_interval 1000 --save_interval 10000 --eval_interval 100002 --model.warmup 1000 --model.use_cosine 1 --save_dir ckpts7T/cifar10-icfm-ft --load_dir ckpts7T/cifar10-icfm-scratch/100000.pkl
CUDA_VISIBLE_DEVICES=2,3, python train.py --dataset_name cifar10 --batch_size 256 --max_steps 20_000 --model.train_type   fm  --model.use_stable_vae 0 --log_interval 1000 --save_interval 10000 --eval_interval 100002 --model.warmup 1000 --model.use_cosine 1 --save_dir ckpts7T/cifar10-fm-ft --load_dir ckpts7T/cifar10-fm-scratch/100000.pkl
CUDA_VISIBLE_DEVICES=6,7, python train.py --dataset_name cifar10 --batch_size 256 --max_steps 20_000 --model.train_type naive --model.use_stable_vae 0 --log_interval 1000 --save_interval 10000 --eval_interval 100002 --model.warmup 1000 --model.use_cosine 1 --save_dir ckpts7T/cifar10-naive128-ft --load_dir ckpts7T/cifar10-naive128-scratch/100000.pkl --model.denoise_timesteps 128

CUDA_VISIBLE_DEVICES=1, python train.py --dataset_name cifar10 --batch_size 256 --mode eval --model.train_type   rfm --model.use_stable_vae 0 --model.use_ema 1 --inference_solver dopri5 --inference_stepsize pid --inference_tol 1e-3 --inference_generations 50000 --load_dir ckpts7T/cifar10-rfm-ft/20000.pkl      --fid_stats data/cifar10_fidstats_jax.npz 
CUDA_VISIBLE_DEVICES=1, python train.py --dataset_name cifar10 --batch_size 256 --mode eval --model.train_type   rfm --model.use_stable_vae 0 --model.use_ema 1 --inference_solver dopri5 --inference_stepsize pid --inference_tol 1e-3 --inference_generations 16384 --load_dir ckpts7T/cifar10-rfm-ft/20000.pkl      --fid_stats data/cifar10_fidstats_jax.npz 


# BASELINES
CELEB/CIFAR10 BASE:
CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --dataset_name cifar10         --fid_stats data/cifar10_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/cifar10-shortcut-base  --max_steps 500_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --dataset_name cifar10         --fid_stats data/cifar10_fidstats_jax.npz --batch_size 128 --mode train --model.train_type      rfm --save_dir ckpts7T/cifar10-rfm-cont-base  --max_steps 500_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0 --model.t_sampling continuous
CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --dataset_name cifar10         --fid_stats data/cifar10_fidstats_jax.npz --batch_size 128 --mode train --model.train_type    naive --save_dir ckpts7T/cifar10-rfm-naive-base --max_steps 400_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0 --model.t_sampling discrete
CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --dataset_name cifar10     -model.lr 2e-5 --model.dropout 0.1 --fid_stats data/cifar10_fidstats_jax.npz     --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/cifar10-shortcut-ft  --load_dir ckpts7T/cifar10-shortcut-base/250000.pkl  --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0
CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --dataset_name cifar10     -model.lr 2e-5 --model.dropout 0.1 --fid_stats data/cifar10_fidstats_jax.npz     --batch_size 128 --mode train --model.train_type    naive --save_dir ckpts7T/cifar10-rfm-naive-ft --load_dir ckpts7T/cifar10-rfm-naive-base/250000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0 --model.t_sampling discrete
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --dataset_name cifar10     -model.lr 2e-5 --model.dropout 0.1 --fid_stats data/cifar10_fidstats_jax.npz     --batch_size 128 --mode train --model.train_type      rfm --save_dir ckpts7T/cifar10-rfm-cont-ft  --load_dir ckpts7T/cifar10-rfm-cont-base/250000.pkl  --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0 --model.t_sampling continuous
CELEB OURS:
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-base   --max_steps 400_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000
CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type      rfm --save_dir ckpts7T/celeba-rfm-cont-base   --max_steps 400_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.t_sampling continuous
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --dataset_name celebahq256 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type    naive --save_dir ckpts7T/celeba-rfm-naive-base  --max_steps 400_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete

DONE: CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py                       --dataset_name celebahq256 -model.lr 2e-5 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type      rfm --save_dir ckpts7T/celeba-rfm-cont-ft      --load_dir ckpts7T/celeba-rfm-cont-base/100000.pkl   --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling continuous
DONE: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py                       --dataset_name celebahq256 -model.lr 2e-5 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type    naive --save_dir ckpts7T/celeba-rfm-naive-ft     --load_dir ckpts7T/celeba-rfm-naive-base/100000.pkl  --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete
DONE: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type    naive --save_dir ckpts7T/celeba-rfm-naive-ours   --load_dir ckpts7T/celeba-rfm-naive-base/100000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete
DONE: CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type      rfm --save_dir ckpts7T/celeba-rfm-cont-ours    --load_dir ckpts7T/celeba-rfm-cont-base/100000.pkl  --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling continuous

DONE: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py                       --dataset_name celebahq256 -model.lr 2e-5 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ft      --load_dir ckpts7T/celeba-shortcut-base/100000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete
TODO: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py                       --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ft2     --load_dir ckpts7T/celeba-shortcut-base/100000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete

DONE: CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ours-G1 --load_dir ckpts7T/celeba-shortcut-base/100000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_group 1
DONE: CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ours-G2 --load_dir ckpts7T/celeba-shortcut-base/100000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_group 2
DONE: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ours-G4 --load_dir ckpts7T/celeba-shortcut-base/100000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_group 4

DONE: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_bootstrap 4 --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ours-G4-BST4 --load_dir ckpts7T/celeba-shortcut-base/100000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_group 4
DONE: CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.depth_bootstrap 0 --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ours-G4-BST0 --load_dir ckpts7T/celeba-shortcut-base/100000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_group 4
CIFAR10 OURS:
DONE: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_wise 12 --dataset_name cifar10     -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/cifar10_fidstats_jax.npz     --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/cifar10-shortcut-ours    --load_dir ckpts7T/cifar10-shortcut-base/250000.pkl  --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0
DONE: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_wise 12 --dataset_name cifar10     -model.lr 2e-5 --model.dropout 0.1 --fid_stats data/cifar10_fidstats_jax.npz     --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/cifar10-shortcut-ours-ft --load_dir ckpts7T/cifar10-shortcut-ours/100000.pkl  --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_wise 12 --dataset_name cifar10     -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/cifar10_fidstats_jax.npz     --batch_size 128 --mode train --model.train_type    naive --save_dir ckpts7T/cifar10-rfm-naive-ours --load_dir ckpts7T/cifar10-rfm-naive-base/250000.pkl --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0 --model.t_sampling discrete
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_wise 12 --dataset_name cifar10     -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/cifar10_fidstats_jax.npz     --batch_size 128 --mode train --model.train_type      rfm --save_dir ckpts7T/cifar10-rfm-cont-ours  --load_dir ckpts7T/cifar10-rfm-cont-base/250000.pkl  --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.use_stable_vae 0 --model.t_sampling continuous


TODO:
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ours2-G4 --load_dir ckpts7T/celeba-shortcut-ours-G4/50000.pkl --max_steps 50_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_group 4
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ours3-G4 --load_dir ckpts7T/celeba-shortcut-ours-G4/50000.pkl --max_steps 50_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_group 4
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.depth_wise 12 --dataset_name celebahq256 -model.lr 1e-4 --model.dropout 0.1 --fid_stats data/celebahq256_fidstats_jax.npz --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ours4-G4 --load_dir ckpts7T/celeba-shortcut-ours-G4/50000.pkl --max_steps 50_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_group 4


ImageNet BASE DiT-B:
python train.py --model.hidden_size 768  --model.patch_size 2 --model.depth 12 --model.num_heads 12 --model.mlp_ratio 4 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --model.cfg_scale 1.5 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 256 --max_steps 810_000 --model.train_type shortcut

CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.lr 1e-4 --model.dropout 0.1 --model.hidden_size 768 --model.patch_size 2 --model.depth_wise  0 --model.depth_group 12 --model.num_heads 12 --model.mlp_ratio 4 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --load_dir ckpts/imagenet-shortcut2-b-fulldata800001 --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ft   --max_steps 500_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete
NOW: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.class_dropout_prob 0.1 --model.lr 2e-5 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --load_dir ckpts/imagenet-shortcut2-b-fulldata800001 --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ft      --max_steps 500_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete
NOW: CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.class_dropout_prob 0.1 --model.lr 2e-5 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --load_dir ckpts/imagenet-shortcut2-b-fulldata800001 --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ours    --max_steps 500_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_wise 12 --model.depth_group 4
NOW: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.class_dropout_prob 0.0 --model.lr 2e-5 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --load_dir ckpts/imagenet-shortcut2-b-fulldata800001 --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ours-G2 --max_steps 500_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_wise 12 --model.depth_group 2
NOW: CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.class_dropout_prob 0.1 --model.lr 1e-6 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --load_dir ckpts7T/imagenet-shortcut-ft/250000.pkl   --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ft      --max_steps 500_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete
NOW: CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.class_dropout_prob 0.1 --model.lr 1e-6 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --load_dir ckpts7T/imagenet-shortcut-ours/500000.pkl --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ours-G4 --max_steps 500_000 --eval_interval 50000 --save_interval 50000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_wise 12 --model.depth_group 4

CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.class_dropout_prob 0.1 --model.lr 1e-6 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --load_dir ckpts7T/imagenet-shortcut-ours-G4/250000.pkl --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ours2-G4 --max_steps 50_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_wise 12 --model.depth_group 4
CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.class_dropout_prob 0.1 --model.lr 1e-6 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --load_dir ckpts7T/imagenet-shortcut-ours-G4/250000.pkl --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ours3-G4 --max_steps 50_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_wise 12 --model.depth_group 4
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.class_dropout_prob 0.1 --model.lr 1e-6 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --load_dir ckpts7T/imagenet-shortcut-ours-G4/250000.pkl --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ours4-G4 --max_steps 50_000 --eval_interval 25000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.depth_wise 12 --model.depth_group 4



ImageNet BASE DiT-XL:
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train.py --model.class_dropout_prob 0.1 --model.lr 2e-5 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 64 --mode train --model.train_type shortcut --save_dir ckpts7Txlimagenet-sm-ft 
--max_steps 100_000 --eval_interval 250000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.hidden_size 1152 --model.depth 28 --model.num_heads 16 --load_dir ckpts/imagenet-shortcut2-xl-fulldata-continue200000

CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train.py --model.class_dropout_prob 0.1 --model.lr 1e-4 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 64 --mode train --model.train_type shortcut --save_dir ckpts7T/xlimagenet-ours    --max_steps 50_000  --eval_interval 250000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.hidden_size 1152 --model.depth 28 --model.num_heads 16 --load_dir ckpts/imagenet-shortcut2-xl-fulldata-continue200000 --model.depth_wise 28 --model.depth_group 4
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train.py --model.class_dropout_prob 0.1 --model.lr 1e-5 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 64 --mode train --model.train_type shortcut --save_dir ckpts7T/xlimagenet-ours-ft --max_steps 50_000  --eval_interval 250000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.hidden_size 1152 --model.depth 28 --model.num_heads 16 --load_dir ckpts7T/xlimagenet-ours/50000.pkl --model.depth_wise 28 --model.depth_group 4
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train.py --model.class_dropout_prob 0.1 --model.lr 2e-5 --dataset_name imagenet256 --fid_stats data/imagenet256_fidstats_ours.npz --model.cfg_scale 1.5 --model.num_classes 1000 --model.bootstrap_cfg 1 --batch_size 64 --mode train --model.train_type shortcut --save_dir ckpts7T/xlimagenet-ours-ft2 --max_steps 50_000  --eval_interval 250000 --save_interval 25000 --log_interval 5000 --model.warmup 5000 --model.t_sampling discrete --model.hidden_size 1152 --model.depth 28 --model.num_heads 16 --load_dir ckpts7T/xlimagenet-ours-ft/50000.pkl --model.depth_wise 28 --model.depth_group 4


NEW experiments:
python train.py --model.lr 1e-4 --model.dropout 0.1 --model.depth_wise 12 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/ourimgnet-shortcut-base --max_steps 500_000 --eval_interval 2500000 --save_interval 50000
python train.py --model.lr 1e-5 --model.dropout 0.1 --model.depth_wise 12 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/ourimgnet-shortcut-ft   --max_steps 500_000 --eval_interval 2500000 --save_interval 50000 --load_dir ckpts7T/ourimgnet-shortcut-base/500000.pkl
python train.py --model.lr 1e-6 --model.dropout 0.0 --model.depth_wise 12 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/ourimgnet-shortcut-ft2  --max_steps 100_000 --eval_interval 2500000 --save_interval 50000 --load_dir ckpts7T/ourimgnet-shortcut-ft/500000.pkl

# DEBUG USING SM:
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.bootstrap_cfg 0 --model.lr 1e-6 --model.dropout 0.0 --model.depth_wise 0 --model.cfg_scale 1.5 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-bs0 --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --load_dir ckpts7T/imagenet-shortcut-ft/250000.pkl --fid_stats data/imagenet256_fidstats_ours.npz
CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.bootstrap_cfg 1 --model.lr 1e-6 --model.dropout 0.0 --model.depth_wise 0 --model.cfg_scale 1.5 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-bs1 --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --load_dir ckpts7T/imagenet-shortcut-ft/250000.pkl --fid_stats data/imagenet256_fidstats_ours.npz
DIFF: --model.bootstrap_cfg 1 vs 0 !!!!!! -> 1 is the best

CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.bootstrap_cfg 0 --model.lr 1e-6 --model.dropout 0.0 --model.depth_wise 0 --model.cfg_scale 1.5 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-bs1-determ-clip --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --load_dir ckpts7T/imagenet-shortcut-ft/250000.pkl --fid_stats data/imagenet256_fidstats_ours.npz
CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.bootstrap_cfg 1 --model.lr 1e-6 --model.dropout 0.0 --model.depth_wise 0 --model.cfg_scale 1.5 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-bs1-determ      --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --load_dir ckpts7T/imagenet-shortcut-ft/250000.pkl --fid_stats data/imagenet256_fidstats_ours.npz

python train.py --model.bootstrap_cfg 1 --model.lr 1e-6 --model.dropout 0.0 --model.depth_wise 0 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-bs1-ourmod --max_steps 50_000 --eval_interval 250000 --save_interval 50000 --load_dir ckpts7T/imagenet-shortcut-ft/250000.pkl --fid_stats data/imagenet256_fidstats_ours.npz

python train.py --model.bootstrap_cfg 1 --model.lr 1e-4 --model.dropout 0.1 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ourmod     --max_steps 500_000 --eval_interval 5000000 --save_interval 50000 --load_dir ckpts/imagenet-shortcut2-b-fulldata800001      --fid_stats data/imagenet256_fidstats_ours.npz
python train.py --model.bootstrap_cfg 1 --model.lr 1e-5 --model.dropout 0.0 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ourmod-ft  --max_steps 500_000 --eval_interval 5000000 --save_interval 50000 --load_dir ckpts7T/imagenet-shortcut-ourmod/300000.pkl    --fid_stats data/imagenet256_fidstats_ours.npz
python train.py --model.bootstrap_cfg 1 --model.lr 1e-6 --model.dropout 0.0 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ourmod-ft2 --max_steps 50_000  --eval_interval 5000000 --save_interval 50000 --load_dir ckpts7T/imagenet-shortcut-ourmod-ft/100000.pkl --fid_stats data/imagenet256_fidstats_ours.npz


CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.bootstrap_cfg 0 --model.lr 1e-4 --model.dropout 0.1 --model.class_dropout_prob 1.0 --model.num_classes    1 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale   0 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/celeba-shortcut-ournew-G4   --load_dir ckpts7T/celeba-shortcut-base/100000.pkl   --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --fid_stats data/celebahq256_fidstats_jax.npz  --dataset_name celebahq256

CUDA_VISIBLE_DEVICES=2,3,6,7 python train.py --model.bootstrap_cfg 1 --model.lr 1e-4 --model.dropout 0.1 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ournew-G4   --load_dir ckpts/imagenet-shortcut2-b-fulldata800001 --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256
# NOW:
CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py --model.bootstrap_cfg 1 --model.lr 1e-4 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ournew-G4-1 --load_dir ckpts/imagenet-shortcut2-b-fulldata800001 --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256

# BEST w 128:
python train.py --model.bootstrap_cfg 1 --model.lr 1e-4 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 128 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ournew-G4-1 --load_dir ckpts/imagenet-shortcut2-b-fulldata800001 --max_steps 100_000 --eval_interval 25000 --save_interval 25000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256
# NOW w 256:
python train.py --model.bootstrap_cfg 1 --model.lr 1e-4 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ournew-G4-1-256 --load_dir ckpts/imagenet-shortcut2-b-fulldata800001            --max_steps 100_000 --eval_interval 250000 --save_interval 25000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256
python train.py --model.bootstrap_cfg 1 --model.lr 1e-5 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ournew-G4-256ft --load_dir ckpts7T/imagenet-shortcut-ournew-G4-1-256/100000.pkl --max_steps 100_000 --eval_interval 250000 --save_interval 25000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256
python train.py --seed  0 --model.bootstrap_cfg 1 --model.lr 1e-5 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ournew-G4-256ft2 --load_dir ckpts7T/imagenet-shortcut-ournew-G4-1-256/100000.pkl --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256
python train.py --seed 20 --model.bootstrap_cfg 1 --model.lr 1e-5 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ournew-G4-256ft3 --load_dir ckpts7T/imagenet-shortcut-ournew-G4-1-256/100000.pkl --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256
python train.py --seed 30 --model.bootstrap_cfg 1 --model.lr 1e-5 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 12 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-shortcut-ournew-G4-256ft4 --load_dir ckpts7T/imagenet-shortcut-ournew-G4-1-256/100000.pkl --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256

# BASE DiT-B:
python train.py --model.bootstrap_cfg 1 --model.lr 1e-4 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 0 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-smnew    --load_dir ckpts/imagenet-shortcut2-b-fulldata800001 --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256
python train.py --model.bootstrap_cfg 1 --model.lr 1e-5 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 0 --model.depth_group 4 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts7T/imagenet-smnew-ft --load_dir ckpts7T/imagenet-smnew/100000.pkl         --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256


# DiT-XL
python train.py --model.bootstrap_cfg 1 --model.lr 1e-4 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise 28 --model.depth_group 8 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts/xlimagenet-shortcut-ours --load_dir ckpts/imagenet-shortcut2-xl-fulldata-continue200000 --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256 --model.hidden_size 1152 --model.depth 28 --model.num_heads 16 --model.depth_min 12
python train.py --model.bootstrap_cfg 1 --model.lr 1e-4 --model.dropout 0.0 --model.class_dropout_prob 0.1 --model.num_classes 1000 --model.depth_wise  0 --model.depth_group 8 --model.cfg_scale 1.5 --batch_size 256 --mode train --model.train_type shortcut --save_dir ckpts/xlimagenet-shortcut-sm   --load_dir ckpts/imagenet-shortcut2-xl-fulldata-continue200000 --max_steps 100_000 --eval_interval 250000 --save_interval 50000 --fid_stats data/imagenet256_fidstats_ours.npz --dataset_name imagenet256 --model.hidden_size 1152 --model.depth 28 --model.num_heads 16 --model.depth_min 12


```